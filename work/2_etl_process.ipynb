{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25ba22fd-fd91-490d-9f53-3c016babead0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INICIANDO PROCESO ETL (MODO FULL REFRESH) ---\n",
      "\n",
      ">>> 0. Limpiando Data Warehouse (Full Refresh)...\n",
      "   -> Borrando FACT_PAGO...\n",
      "   -> Borrando Dimensiones...\n",
      "# DW Limpiado correctamente. Listo subir los datos.\n",
      "\n",
      ">>> 1. Procesando Dimensiones...\n",
      "   -> Extrayendo CLIENTE...\n",
      "   -> Cargando DIM_CLIENTE...\n",
      "   -> Extrayendo HABITACION...\n",
      "   -> Cargando DIM_HABITACION...\n",
      "   -> Extrayendo PROPIETARIO...\n",
      "   -> Cargando DIM_PROPIETARIO...\n",
      "\n",
      ">>> 2. Procesando Dimensión Tiempo...\n",
      "   -> Extrayendo PAGO...\n",
      "   -> Cargando DIM_TIEMPO...\n",
      "\n",
      ">>> 3. Procesando Hechos (FACT_PAGO)...\n",
      "   -> Extrayendo CONTRATO...\n",
      "   -> Cargando FACT_PAGO...\n",
      "\n",
      "# ETL Finalizado Correctamente -> DW Actualizado\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year, month, dayofmonth, to_date\n",
    "\n",
    "# --- CONFIGURACIÓN CENTRALIZADA ---\n",
    "DB_HOST = \"sql_server\"\n",
    "DB_PORT = \"1433\"\n",
    "DB_USER = \"sa\"\n",
    "DB_PASS = \"PasswordFuerte123!\" \n",
    "DRIVER_PKG = \"com.microsoft.sqlserver:mssql-jdbc:11.2.0.jre8\"\n",
    "DRIVER_CLASS = \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "\n",
    "# Bases de datos\n",
    "DB_OLTP = \"alquiler_habitacion\"\n",
    "DB_DW = \"DW_AlquilerHabitacion\"\n",
    "\n",
    "print(\"--- INICIANDO PROCESO ETL (MODO FULL REFRESH) ---\")\n",
    "\n",
    "# Iniciar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL-Alquileres\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", DRIVER_PKG) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Funciones de Lectura y Escritura\n",
    "def get_jdbc_url(db_name):\n",
    "    return f\"jdbc:sqlserver://{DB_HOST}:{DB_PORT};databaseName={db_name};encrypt=true;trustServerCertificate=true;\"\n",
    "\n",
    "def leer_origen(table_name):\n",
    "    print(f\"   -> Extrayendo {table_name}...\")\n",
    "    return spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", get_jdbc_url(DB_OLTP)) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", DB_USER) \\\n",
    "        .option(\"password\", DB_PASS) \\\n",
    "        .option(\"driver\", DRIVER_CLASS) \\\n",
    "        .load()\n",
    "\n",
    "def escribir_destino(df, table_name):\n",
    "    print(f\"   -> Cargando {table_name}...\")\n",
    "    df.write.format(\"jdbc\") \\\n",
    "        .option(\"url\", get_jdbc_url(DB_DW)) \\\n",
    "        .option(\"dbtable\", table_name) \\\n",
    "        .option(\"user\", DB_USER) \\\n",
    "        .option(\"password\", DB_PASS) \\\n",
    "        .option(\"driver\", DRIVER_CLASS) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "# Función de Limpieza\n",
    "def limpiar_dw():\n",
    "    print(\"\\n>>> 0. Limpiando Data Warehouse (Full Refresh)...\")\n",
    "    url = get_jdbc_url(DB_DW)\n",
    "    try:\n",
    "        # REGISTRO MANUAL DEL DRIVER (Vital para comandos SQL directos)\n",
    "        # Accedemos a la clase del driver directamente desde el Gateway de Spark\n",
    "        driver_instance = spark.sparkContext._gateway.jvm.com.microsoft.sqlserver.jdbc.SQLServerDriver()\n",
    "        spark.sparkContext._gateway.jvm.java.sql.DriverManager.registerDriver(driver_instance)\n",
    "        \n",
    "        manager = spark.sparkContext._gateway.jvm.java.sql.DriverManager\n",
    "        con = manager.getConnection(url, DB_USER, DB_PASS)\n",
    "        stmt = con.createStatement()\n",
    "        \n",
    "        # Orden estricto: Primero Hechos (Hijos), luego Dimensiones (Padres)\n",
    "        print(\"   -> Borrando FACT_PAGO...\")\n",
    "        stmt.execute(\"DELETE FROM FACT_PAGO\")\n",
    "        stmt.execute(\"DBCC CHECKIDENT ('FACT_PAGO', RESEED, 0)\") # Reinicia IDs a 0\n",
    "        \n",
    "        print(\"   -> Borrando Dimensiones...\")\n",
    "        stmt.execute(\"DELETE FROM DIM_CLIENTE\")\n",
    "        stmt.execute(\"DBCC CHECKIDENT ('DIM_CLIENTE', RESEED, 0)\")\n",
    "        \n",
    "        stmt.execute(\"DELETE FROM DIM_HABITACION\")\n",
    "        stmt.execute(\"DBCC CHECKIDENT ('DIM_HABITACION', RESEED, 0)\")\n",
    "        \n",
    "        stmt.execute(\"DELETE FROM DIM_PROPIETARIO\")\n",
    "        stmt.execute(\"DBCC CHECKIDENT ('DIM_PROPIETARIO', RESEED, 0)\")\n",
    "        \n",
    "        stmt.execute(\"DELETE FROM DIM_TIEMPO\")\n",
    "        stmt.execute(\"DBCC CHECKIDENT ('DIM_TIEMPO', RESEED, 0)\")\n",
    "        \n",
    "        stmt.close()\n",
    "        con.close()\n",
    "        print(\"# DW Limpiado correctamente. Listo subir los datos.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ ERROR CRÍTICO AL LIMPIAR: {e}\")\n",
    "        print(\"   ⛔ El proceso se detendrá para evitar duplicar datos.\")\n",
    "        raise e # Detenemos el script. Si no limpiamos, NO cargamos.\n",
    "\n",
    "# ==========================================\n",
    "# EJECUCIÓN DEL FLUJO\n",
    "# ==========================================\n",
    "\n",
    "# LIMPIEZA AUTOMÁTICA\n",
    "limpiar_dw()\n",
    "\n",
    "print(\"\\n>>> 1. Procesando Dimensiones...\")\n",
    "\n",
    "# CLIENTE\n",
    "df_cliente = leer_origen(\"CLIENTE\")\n",
    "df_cliente_dw = df_cliente.select(\"idCliente\", \"nombreCli\", \"dniCli\", \"telefonoCli\", \"correoCli\", \"direccionCli\")\n",
    "escribir_destino(df_cliente_dw, \"DIM_CLIENTE\")\n",
    "\n",
    "# HABITACION\n",
    "df_habitacion = leer_origen(\"HABITACION\")\n",
    "df_hab_dw = df_habitacion.select(\"idHabitacion\", \"numeroHabitacion\", \"tipoHabitacion\", \"precioMensual\", \"estado\")\n",
    "escribir_destino(df_hab_dw, \"DIM_HABITACION\")\n",
    "\n",
    "# PROPIETARIO\n",
    "df_propietario = leer_origen(\"PROPIETARIO\")\n",
    "df_prop_dw = df_propietario.select(\"idPropietario\", \"nombrePropietario\", \"correoPropietario\")\n",
    "escribir_destino(df_prop_dw, \"DIM_PROPIETARIO\")\n",
    "\n",
    "# DIMENSIÓN TIEMPO\n",
    "print(\"\\n>>> 2. Procesando Dimensión Tiempo...\")\n",
    "df_pagos_src = leer_origen(\"PAGO\")\n",
    "\n",
    "# Extraer fechas únicas de los pagos\n",
    "df_fechas = df_pagos_src.select(to_date(col(\"fechaPago\")).alias(\"fecha\")).distinct()\n",
    "\n",
    "df_tiempo = df_fechas.select(\n",
    "    col(\"fecha\"),\n",
    "    year(col(\"fecha\")).alias(\"anio\"),\n",
    "    month(col(\"fecha\")).alias(\"mes\"),\n",
    "    dayofmonth(col(\"fecha\")).alias(\"dia\")\n",
    ")\n",
    "escribir_destino(df_tiempo, \"DIM_TIEMPO\")\n",
    "\n",
    "# FACT_PAGO\n",
    "print(\"\\n>>> 3. Procesando Hechos (FACT_PAGO)...\")\n",
    "\n",
    "# Se Lee las Dimensiones del DW\n",
    "def leer_dw(table):\n",
    "    return spark.read.format(\"jdbc\") \\\n",
    "        .option(\"url\", get_jdbc_url(DB_DW)).option(\"dbtable\", table) \\\n",
    "        .option(\"user\", DB_USER).option(\"password\", DB_PASS).option(\"driver\", DRIVER_CLASS).load()\n",
    "\n",
    "dim_cliente = leer_dw(\"DIM_CLIENTE\").select(\"idCliente\", \"idClienteDW\")\n",
    "dim_habitacion = leer_dw(\"DIM_HABITACION\").select(\"idHabitacion\", \"idHabitacionDW\")\n",
    "dim_propietario = leer_dw(\"DIM_PROPIETARIO\").select(\"idPropietario\", \"idPropietarioDW\")\n",
    "dim_tiempo = leer_dw(\"DIM_TIEMPO\").select(\"fecha\", \"idTiempoDW\")\n",
    "\n",
    "# Se prepara la Transacción Completa\n",
    "df_contrato = leer_origen(\"CONTRATO\")\n",
    "\n",
    "# Se unen los Pago con Contrato para saber quién y qué\n",
    "df_transaccion = df_pagos_src.alias(\"p\") \\\n",
    "    .join(df_contrato.alias(\"c\"), col(\"p.idContrato\") == col(\"c.idContrato\")) \\\n",
    "    .select(\n",
    "        col(\"p.fechaPago\"),\n",
    "        col(\"p.montoPago\"),\n",
    "        col(\"p.metodoPago\"),\n",
    "        col(\"c.idCliente\"),\n",
    "        col(\"c.idHabitacion\"),\n",
    "        col(\"c.idPropietario\")\n",
    "    )\n",
    "\n",
    "# Sustitución de IDs viejos por IDs del DW\n",
    "df_fact = df_transaccion \\\n",
    "    .join(dim_cliente, \"idCliente\") \\\n",
    "    .join(dim_habitacion, \"idHabitacion\") \\\n",
    "    .join(dim_propietario, \"idPropietario\") \\\n",
    "    .join(dim_tiempo, df_transaccion.fechaPago == dim_tiempo.fecha) \\\n",
    "    .select(\n",
    "        col(\"idClienteDW\"),\n",
    "        col(\"idHabitacionDW\"),\n",
    "        col(\"idPropietarioDW\"),\n",
    "        col(\"idTiempoDW\"),\n",
    "        col(\"montoPago\"),\n",
    "        col(\"metodoPago\")\n",
    "    )\n",
    "\n",
    "escribir_destino(df_fact, \"FACT_PAGO\")\n",
    "\n",
    "print(\"\\n# ETL Finalizado Correctamente -> DW Actualizado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43f4ef-2b02-47a8-9ab4-7c5303a8b56c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
